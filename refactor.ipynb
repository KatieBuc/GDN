{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19ffd3ed910>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from datasets.TimeDataset import TimeDataset\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDN(nn.Module):\n",
    "    def __init__(self,\n",
    "    edge_index_sets,\n",
    "    node_num,\n",
    "    dim=64, out_layer_inter_dim=256, input_dim=10, out_layer_num=1, topk=20):\n",
    "\n",
    "        super(GDN, self).__init__()\n",
    "\n",
    "        self.edge_index_sets = edge_index_sets\n",
    "\n",
    "        device = get_device()\n",
    "\n",
    "        edge_index = edge_index_sets[0]\n",
    "\n",
    "\n",
    "        embed_dim = dim\n",
    "        self.embedding = nn.Embedding(node_num, embed_dim)\n",
    "        self.bn_outlayer_in = nn.BatchNorm1d(embed_dim)\n",
    "\n",
    "\n",
    "        edge_set_num = len(edge_index_sets)\n",
    "        self.gnn_layers = nn.ModuleList([\n",
    "            GNNLayer(input_dim, dim, inter_dim=dim+embed_dim, heads=1) for i in range(edge_set_num)\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.node_embedding = None\n",
    "        self.topk = topk\n",
    "        self.learned_graph = None\n",
    "\n",
    "        self.out_layer = OutLayer(dim*edge_set_num, node_num, out_layer_num, inter_num = out_layer_inter_dim)\n",
    "\n",
    "        self.cache_edge_index_sets = [None] * edge_set_num\n",
    "        self.cache_embed_index = None\n",
    "\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "\n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self):\n",
    "        nn.init.kaiming_uniform_(self.embedding.weight, a=math.sqrt(5))\n",
    "\n",
    "\n",
    "    def forward(self, data, org_edge_index):\n",
    "\n",
    "        x = data.clone().detach()\n",
    "        edge_index_sets = self.edge_index_sets\n",
    "\n",
    "        device = data.device\n",
    "\n",
    "        batch_num, node_num, all_feature = x.shape\n",
    "        x = x.view(-1, all_feature).contiguous()\n",
    "\n",
    "\n",
    "        gcn_outs = []\n",
    "        for i, edge_index in enumerate(edge_index_sets):\n",
    "            edge_num = edge_index.shape[1]\n",
    "            cache_edge_index = self.cache_edge_index_sets[i]\n",
    "\n",
    "            if cache_edge_index is None or cache_edge_index.shape[1] != edge_num*batch_num:\n",
    "                self.cache_edge_index_sets[i] = get_batch_edge_index(edge_index, batch_num, node_num).to(device)\n",
    "            \n",
    "            batch_edge_index = self.cache_edge_index_sets[i]\n",
    "            \n",
    "            all_embeddings = self.embedding(torch.arange(node_num).to(device)) # v_i's\n",
    "\n",
    "            weights_arr = all_embeddings.detach().clone()\n",
    "            all_embeddings = all_embeddings.repeat(batch_num, 1)\n",
    "\n",
    "            weights = weights_arr.view(node_num, -1)\n",
    "\n",
    "            cos_ji_mat = torch.matmul(weights, weights.T) # e_{ji} in eqn (2)\n",
    "            normed_mat = torch.matmul(weights.norm(dim=-1).view(-1,1), weights.norm(dim=-1).view(1,-1))\n",
    "            cos_ji_mat = cos_ji_mat / normed_mat \n",
    "\n",
    "            dim = weights.shape[-1]\n",
    "            topk_num = self.topk\n",
    "\n",
    "            topk_indices_ji = torch.topk(cos_ji_mat, topk_num, dim=-1)[1] # A_{ji} in eqn (3)\n",
    "\n",
    "            self.learned_graph = topk_indices_ji \n",
    "\n",
    "            gated_i_ = torch.arange(0, node_num)\n",
    "            gated_i = gated_i_.permute(*torch.arange(gated_i_.ndim - 1, -1, -1)).unsqueeze(1).repeat(1, topk_num).flatten().to(device).unsqueeze(0)\n",
    "            #gated_i = torch.arange(0, node_num).T.unsqueeze(1).repeat(1, topk_num).flatten().to(device).unsqueeze(0)\n",
    "            gated_j = topk_indices_ji.flatten().unsqueeze(0)\n",
    "            gated_edge_index = torch.cat((gated_j, gated_i), dim=0)\n",
    "\n",
    "            batch_gated_edge_index = get_batch_edge_index(gated_edge_index, batch_num, node_num).to(device)\n",
    "\n",
    "            gcn_out = self.gnn_layers[i](x, batch_gated_edge_index, node_num=node_num*batch_num, embedding=all_embeddings)\n",
    "\n",
    "\n",
    "            gcn_outs.append(gcn_out)\n",
    "\n",
    "        x = torch.cat(gcn_outs, dim=1)\n",
    "        x = x.view(batch_num, node_num, -1)\n",
    "\n",
    "\n",
    "        indexes = torch.arange(0,node_num).to(device)\n",
    "        out = torch.mul(x, self.embedding(indexes))\n",
    "\n",
    "\n",
    "        \n",
    "        out = out.permute(0,2,1)\n",
    "        out = F.relu(self.bn_outlayer_in(out)) # eqn (5)\n",
    "        out = out.permute(0,2,1)\n",
    "\n",
    "        out = self.dp(out)\n",
    "        out = self.out_layer(out)\n",
    "        out = out.view(-1, node_num)\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNAD():\n",
    "    \"\"\"\n",
    "    Graph Neural Network-based Anomaly Detection in Multivariate Timeseries.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        batch: int = 128,\n",
    "        epoch: int = 100,\n",
    "        slide_win: int = 15,\n",
    "        dim: int = 64,\n",
    "        slide_stride: int = 5,\n",
    "        comment: str = '',\n",
    "        random_seed: int = 0,\n",
    "        out_layer_num: int = 1,\n",
    "        out_layer_inter_dim: int = 256,\n",
    "        decay: float = 0,\n",
    "        validate_ratio: float = 0.1,\n",
    "        topk: int = 20,\n",
    "        save_path_pattern: str = 'msl',\n",
    "        dataset: str = 'msl',\n",
    "        device: str = 'cpu',\n",
    "        report: str = 'best',\n",
    "        load_model_path: str = '',\n",
    "        ):\n",
    "\n",
    "        self.batch = batch\n",
    "        self.epoch = epoch\n",
    "        self.slide_win = slide_win\n",
    "        self.dim = dim\n",
    "        self.slide_stride = slide_stride\n",
    "        self.comment = comment\n",
    "        self.random_seed = random_seed\n",
    "        self.out_layer_num = out_layer_num\n",
    "        self.out_layer_inter_dim = out_layer_inter_dim\n",
    "        self.decay = decay\n",
    "        self.validate_ratio = validate_ratio\n",
    "        self.topk = topk\n",
    "        self.save_path_pattern = save_path_pattern\n",
    "        self.dataset = dataset\n",
    "        self.device = device\n",
    "        self.report = report\n",
    "        self.load_model_path = load_model_path\n",
    "\n",
    "\n",
    "    def _split_train_validation(self, data):\n",
    "\n",
    "        dataset_len = len(data)\n",
    "        validate_use_len = int(dataset_len * self.validate_ratio)\n",
    "        validate_start_idx = random.randrange(dataset_len - validate_use_len)\n",
    "        idx = torch.arange(dataset_len)\n",
    "\n",
    "        train_sub_idx = torch.cat([idx[:validate_start_idx], idx[validate_start_idx+validate_use_len:]])\n",
    "        train_subset = Subset(data, train_sub_idx)\n",
    "\n",
    "        validate_sub_idx = idx[validate_start_idx:validate_start_idx+validate_use_len]\n",
    "        validate_subset = Subset(data, validate_sub_idx)\n",
    "\n",
    "        return train_subset, validate_subset\n",
    "\n",
    "\n",
    "    def _load_data(self):\n",
    "\n",
    "        train = pd.read_csv(f'./data/{self.dataset}/train.csv', sep=',', index_col=0)\n",
    "        test = pd.read_csv(f'./data/{self.dataset}/test.csv', sep=',', index_col=0)\n",
    "\n",
    "        train = train.drop(columns=['attack']) if 'attack' in train.columns else train\n",
    "        \n",
    "        feature_list = train.columns[train.columns.str[0] != '_'].to_list() # convention is to pass non-features as '_'\n",
    "        assert len(feature_list) == len(set(feature_list))\n",
    "\n",
    "        fc_struc = {ft: [x for x in feature_list if x != ft] for ft in feature_list} # fully connected structure\n",
    "\n",
    "        edge__idx_tuples = [(feature_list.index(child), feature_list.index(node_name)) \n",
    "        for node_name, node_list in fc_struc.items() for child in node_list]\n",
    "\n",
    "        fc_edge_idx = [[x[0] for x in edge__idx_tuples], [x[1] for x in edge__idx_tuples]]\n",
    "        fc_edge_idx = torch.tensor(fc_edge_idx, dtype = torch.long)\n",
    "\n",
    "        train_input = _parse_data(train, feature_list)\n",
    "        test_input = _parse_data(test, feature_list, labels=test.attack.tolist())\n",
    "\n",
    "        cfg = {\n",
    "            'slide_win': self.slide_win,\n",
    "            'slide_stride': self.slide_stride,\n",
    "        }\n",
    "        \n",
    "        train_dataset = TimeDataset(train_input, fc_edge_idx, mode='train', config=cfg)\n",
    "        test_dataset = TimeDataset(test_input, fc_edge_idx, mode='test', config=cfg)\n",
    "\n",
    "        train_subset, validate_subset = self._split_train_validation(train_dataset)\n",
    "\n",
    "        # get data loaders\n",
    "        train_dataloader = DataLoader(train_subset, batch_size=self.batch,\n",
    "                                shuffle=True)\n",
    "\n",
    "        validate_dataloader = DataLoader(validate_subset, batch_size=self.batch,\n",
    "                                shuffle=False)\n",
    "\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=self.batch,\n",
    "                                shuffle=False, num_workers=0)\n",
    "        \n",
    "        # instantiate model\n",
    "        model = GDN([fc_edge_idx],\n",
    "            len(feature_list), \n",
    "            dim=self.dim, \n",
    "            input_dim=self.slide_win,\n",
    "            out_layer_num=self.out_layer_num,\n",
    "            out_layer_inter_dim=self.out_layer_inter_dim,\n",
    "            topk=self.topk\n",
    "        ).to(self.device)\n",
    "\n",
    "        # save to self\n",
    "        self.feature_list = feature_list\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.validate_dataloader = validate_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self):\n",
    "        self._load_data()\n",
    "\n",
    "\n",
    "\n",
    "def _parse_data(data, feature_list, labels=None):\n",
    "\n",
    "    labels = [0]*data.shape[0] if labels == None else labels\n",
    "    res = data[feature_list].T.values.tolist()\n",
    "    res.append(labels)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNNAD()\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor to module\n",
    "#    - init: dataloaders, GDN\n",
    "#    - run: train, test, check output\n",
    "# write unit tests\n",
    "#--------------------\n",
    "\n",
    "# get interactive validation screen\n",
    "# plots like paper\n",
    "# error handling for real data!\n",
    "\n",
    "#--------------------\n",
    "# ideas for research (graph metrics, input node-related anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('gdn_old')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99f10b4e66e58daffdd3587f8013f35f7438a354bd317537c02f690a0bc2561c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
