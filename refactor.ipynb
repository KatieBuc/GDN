{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\n10907700\\Anaconda3\\envs\\gdn_old\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23f53f9e3f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import iqr, rankdata\n",
    "from sklearn.metrics import (f1_score, precision_score, recall_score,\n",
    "                             roc_auc_score)\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, softmax\n",
    "\n",
    "from datasets.TimeDataset import TimeDataset\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLayer(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        heads=1,\n",
    "        concat=True,\n",
    "        negative_slope=0.2,\n",
    "        dropout=0,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super(GraphLayer, self).__init__(aggr=\"add\")\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.__alpha__ = None\n",
    "\n",
    "        self.lin = Linear(in_channels, heads * out_channels, bias=False)\n",
    "\n",
    "        self.att_i = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_j = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_em_i = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_em_j = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.lin.weight)\n",
    "        glorot(self.att_i)\n",
    "        glorot(self.att_j)\n",
    "        zeros(self.att_em_i)\n",
    "        zeros(self.att_em_j)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, embedding, return_attention_weights=False):\n",
    "\n",
    "        if torch.is_tensor(x):\n",
    "            x = self.lin(x)\n",
    "            x = (x, x)\n",
    "        else:\n",
    "            x = (self.lin(x[0]), self.lin(x[1]))\n",
    "\n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x[1].size(self.node_dim))\n",
    "\n",
    "        out = self.propagate(\n",
    "            edge_index,\n",
    "            x=x,\n",
    "            embedding=embedding,\n",
    "            edges=edge_index,\n",
    "            return_attention_weights=return_attention_weights,\n",
    "        )\n",
    "\n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        if return_attention_weights:\n",
    "            alpha, self.__alpha__ = self.__alpha__, None\n",
    "            return out, (edge_index, alpha)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def message(\n",
    "        self, x_i, x_j, edge_index_i, size_i, embedding, edges, return_attention_weights\n",
    "    ):\n",
    "\n",
    "        x_i = x_i.view(-1, self.heads, self.out_channels)\n",
    "        x_j = x_j.view(-1, self.heads, self.out_channels)\n",
    "\n",
    "        if embedding is not None:\n",
    "            embedding_i, embedding_j = embedding[edge_index_i], embedding[edges[0]]\n",
    "            embedding_i = embedding_i.unsqueeze(1).repeat(1, self.heads, 1)\n",
    "            embedding_j = embedding_j.unsqueeze(1).repeat(1, self.heads, 1)\n",
    "\n",
    "            key_i = torch.cat(\n",
    "                (x_i, embedding_i), dim=-1\n",
    "            )  # key_i's are the g_i's, does x_i already have W?\n",
    "            key_j = torch.cat(\n",
    "                (x_j, embedding_j), dim=-1\n",
    "            )  # concatenates along the last dim, i.e. columns in this case\n",
    "\n",
    "        cat_att_i = torch.cat((self.att_i, self.att_em_i), dim=-1)\n",
    "        cat_att_j = torch.cat((self.att_j, self.att_em_j), dim=-1)\n",
    "\n",
    "        alpha = (key_i * cat_att_i).sum(-1) + (key_j * cat_att_j).sum(\n",
    "            -1\n",
    "        )  # eqn (6) but...\n",
    "        alpha = alpha.view(-1, self.heads, 1)\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)  # eqn (7)\n",
    "        alpha = softmax(alpha, edge_index_i, size_i)  # eqn (8)\n",
    "\n",
    "        if return_attention_weights:\n",
    "            self.__alpha__ = alpha\n",
    "\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "\n",
    "        return x_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}({}, {}, heads={})\".format(\n",
    "            self.__class__.__name__, self.in_channels, self.out_channels, self.heads\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutLayer(nn.Module):\n",
    "    def __init__(self, in_num, layer_num, inter_num=512):\n",
    "\n",
    "        super(OutLayer, self).__init__()\n",
    "        modules = []\n",
    "        for i in range(layer_num):\n",
    "            if i == layer_num - 1:\n",
    "                modules.append(nn.Linear(in_num if layer_num == 1 else inter_num, 1))\n",
    "            else:\n",
    "                layer_in_num = in_num if i == 0 else inter_num\n",
    "                modules.append(nn.Linear(layer_in_num, inter_num))\n",
    "                modules.append(nn.BatchNorm1d(inter_num))\n",
    "                modules.append(nn.ReLU())\n",
    "\n",
    "        self.mlp = nn.ModuleList(modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        for mod in self.mlp:\n",
    "            if isinstance(mod, nn.BatchNorm1d):\n",
    "                out = out.permute(0, 2, 1)\n",
    "                out = mod(out)\n",
    "                out = out.permute(0, 2, 1)\n",
    "            else:\n",
    "                out = mod(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, heads=1):\n",
    "        super(GNNLayer, self).__init__()\n",
    "\n",
    "        self.gnn = GraphLayer(in_channel, out_channel, heads=heads, concat=False)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, embedding=None):\n",
    "\n",
    "        out, (new_edge_index, att_weight) = self.gnn(\n",
    "            x, edge_index, embedding, return_attention_weights=True\n",
    "        )\n",
    "\n",
    "        self.att_weight_1 = att_weight\n",
    "        self.edge_index_1 = new_edge_index\n",
    "\n",
    "        out = self.bn(out)\n",
    "\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class GDN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fc_edge_idx,\n",
    "        n_nodes,\n",
    "        embed_dim=64,\n",
    "        out_layer_inter_dim=256,\n",
    "        input_dim=10,\n",
    "        out_layer_num=1,\n",
    "        topk=20,\n",
    "    ):\n",
    "        super(GDN, self).__init__()\n",
    "\n",
    "        self.fc_edge_idx = fc_edge_idx\n",
    "        self.n_nodes = n_nodes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.out_layer_inter_dim = out_layer_inter_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.out_layer_num = out_layer_num\n",
    "        self.topk = topk\n",
    "\n",
    "    def _initialise_layers(self):\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_nodes, self.embed_dim)\n",
    "        nn.init.kaiming_uniform_(self.embedding.weight, a=math.sqrt(5))\n",
    "\n",
    "        self.bn_outlayer_in = nn.BatchNorm1d(self.embed_dim)\n",
    "\n",
    "        self.gnn_layers = nn.ModuleList(\n",
    "            [\n",
    "                GNNLayer(\n",
    "                    self.input_dim,\n",
    "                    self.embed_dim,\n",
    "                    heads=1,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.node_embedding = None\n",
    "        self.learned_graph = None\n",
    "\n",
    "        self.out_layer = OutLayer(\n",
    "            self.embed_dim, self.out_layer_num, inter_num=self.out_layer_inter_dim\n",
    "        )\n",
    "\n",
    "        self.cache_fc_edge_idx = None\n",
    "        self.cache_embed_index = None\n",
    "\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x = data.clone().detach()\n",
    "        device = data.device\n",
    "        batch_num, n_nodes, all_feature = x.shape\n",
    "        x = x.view(-1, all_feature).contiguous()\n",
    "\n",
    "        if self.cache_fc_edge_idx is None:\n",
    "            self.cache_fc_edge_idx = get_batch_edge_index(\n",
    "                self.fc_edge_idx, batch_num, n_nodes\n",
    "            ).to(device)\n",
    "\n",
    "        all_embeddings = self.embedding(torch.arange(n_nodes).to(device))  # v_i's\n",
    "\n",
    "        weights_arr = all_embeddings.detach().clone()\n",
    "        all_embeddings = all_embeddings.repeat(batch_num, 1)\n",
    "\n",
    "        weights = weights_arr.view(n_nodes, -1)\n",
    "\n",
    "        cos_ji_mat = torch.matmul(weights, weights.T)  # e_{ji} in eqn (2)\n",
    "        normed_mat = torch.matmul(\n",
    "            weights.norm(dim=-1).view(-1, 1), weights.norm(dim=-1).view(1, -1)\n",
    "        )\n",
    "        cos_ji_mat = cos_ji_mat / normed_mat\n",
    "\n",
    "        topk_indices_ji = torch.topk(cos_ji_mat, self.topk, dim=-1)[\n",
    "            1\n",
    "        ]  # A_{ji} in eqn (3)\n",
    "\n",
    "        self.learned_graph = topk_indices_ji\n",
    "\n",
    "        gated_i_ = torch.arange(0, n_nodes)\n",
    "        gated_i = (\n",
    "            gated_i_.permute(*torch.arange(gated_i_.ndim - 1, -1, -1))\n",
    "            .unsqueeze(1)\n",
    "            .repeat(1, self.topk)\n",
    "            .flatten()\n",
    "            .to(device)\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        gated_j = topk_indices_ji.flatten().unsqueeze(0)\n",
    "        gated_edge_index = torch.cat((gated_j, gated_i), dim=0)\n",
    "\n",
    "        batch_gated_edge_index = get_batch_edge_index(\n",
    "            gated_edge_index, batch_num, n_nodes\n",
    "        ).to(device)\n",
    "\n",
    "        gcn_out = self.gnn_layers[0](\n",
    "            x,\n",
    "            batch_gated_edge_index,\n",
    "            embedding=all_embeddings,\n",
    "        )\n",
    "        gcn_out = gcn_out.view(batch_num, n_nodes, -1)\n",
    "\n",
    "        idxs = torch.arange(0, n_nodes).to(device)\n",
    "        out = torch.mul(gcn_out, self.embedding(idxs))\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = F.relu(self.bn_outlayer_in(out))  # eqn (5)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.dp(out)\n",
    "        out = self.out_layer(out)\n",
    "        out = out.view(-1, n_nodes)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_batch_edge_index(org_edge_index, batch_num, n_nodes):\n",
    "\n",
    "    edge_index = org_edge_index.clone().detach()\n",
    "    edge_num = org_edge_index.shape[1]\n",
    "    batch_edge_index = edge_index.repeat(1, batch_num).contiguous()\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        batch_edge_index[:, i * edge_num : (i + 1) * edge_num] += i * n_nodes\n",
    "\n",
    "    return batch_edge_index.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor to module\n",
    "#    - run: get score [today]\n",
    "#    - (deterministic) check output matches, write unit tests [tonight]\n",
    "\n",
    "# --------------------\n",
    "# CELEBRATE!\n",
    "# --------------------\n",
    "\n",
    "# plots like paper\n",
    "# error handling for real data!\n",
    "# get interactive validation screen (notebook)\n",
    "# make this work for new python/torch verison\n",
    "# ...\n",
    "# ideas for research (graph metrics, input node-related anomaly) [meet with Rob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNAD:\n",
    "    \"\"\"\n",
    "    Graph Neural Network-based Anomaly Detection in Multivariate Timeseries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch: int = 128,\n",
    "        epoch: int = 100,\n",
    "        slide_win: int = 15,\n",
    "        dim: int = 64,\n",
    "        slide_stride: int = 5,\n",
    "        comment: str = \"\",\n",
    "        random_seed: int = 0,\n",
    "        out_layer_num: int = 1,\n",
    "        out_layer_inter_dim: int = 256,\n",
    "        decay: float = 0,\n",
    "        validate_ratio: float = 0.1,\n",
    "        topk: int = 20,\n",
    "        data_subdir: str = \"msl\",\n",
    "        device: str = \"cpu\",\n",
    "        report: str = \"best\",\n",
    "        load_model_name: str = \"\",\n",
    "        early_stop_win: int = 15,\n",
    "        lr: float = 0.001,\n",
    "    ):\n",
    "\n",
    "        self.batch = batch\n",
    "        self.epoch = epoch\n",
    "        self.slide_win = slide_win\n",
    "        self.dim = dim\n",
    "        self.slide_stride = slide_stride\n",
    "        self.comment = comment\n",
    "        self.random_seed = random_seed\n",
    "        self.out_layer_num = out_layer_num\n",
    "        self.out_layer_inter_dim = out_layer_inter_dim\n",
    "        self.decay = decay\n",
    "        self.validate_ratio = validate_ratio\n",
    "        self.topk = topk\n",
    "        self.data_subdir = data_subdir\n",
    "        self.device = device\n",
    "        self.report = report\n",
    "        self.load_model_name = load_model_name\n",
    "        self.early_stop_win = early_stop_win\n",
    "        self.lr = lr\n",
    "\n",
    "    def _split_train_validation(self, data):\n",
    "\n",
    "        dataset_len = len(data)\n",
    "        validate_use_len = int(dataset_len * self.validate_ratio)\n",
    "        validate_start_idx = random.randrange(dataset_len - validate_use_len)\n",
    "        idx = torch.arange(dataset_len)\n",
    "\n",
    "        train_sub_idx = torch.cat(\n",
    "            [idx[:validate_start_idx], idx[validate_start_idx + validate_use_len :]]\n",
    "        )\n",
    "        train_subset = Subset(data, train_sub_idx)\n",
    "\n",
    "        validate_sub_idx = idx[\n",
    "            validate_start_idx : validate_start_idx + validate_use_len\n",
    "        ]\n",
    "        validate_subset = Subset(data, validate_sub_idx)\n",
    "\n",
    "        return train_subset, validate_subset\n",
    "\n",
    "    def _load_data(self):\n",
    "\n",
    "        train = pd.read_csv(\n",
    "            f\"./data/{self.data_subdir}/train.csv\", sep=\",\", index_col=0\n",
    "        )\n",
    "        test = pd.read_csv(f\"./data/{self.data_subdir}/test.csv\", sep=\",\", index_col=0)\n",
    "\n",
    "        train = train.drop(columns=[\"attack\"]) if \"attack\" in train.columns else train\n",
    "\n",
    "        feature_list = train.columns[\n",
    "            train.columns.str[0] != \"_\"\n",
    "        ].to_list()  # convention is to pass non-features as '_'\n",
    "        assert len(feature_list) == len(set(feature_list))\n",
    "\n",
    "        fc_struc = {\n",
    "            ft: [x for x in feature_list if x != ft] for ft in feature_list\n",
    "        }  # fully connected structure\n",
    "\n",
    "        edge__idx_tuples = [\n",
    "            (feature_list.index(child), feature_list.index(node_name))\n",
    "            for node_name, node_list in fc_struc.items()\n",
    "            for child in node_list\n",
    "        ]\n",
    "\n",
    "        fc_edge_idx = [\n",
    "            [x[0] for x in edge__idx_tuples],\n",
    "            [x[1] for x in edge__idx_tuples],\n",
    "        ]\n",
    "        fc_edge_idx = torch.tensor(fc_edge_idx, dtype=torch.long)\n",
    "\n",
    "        train_input = parse_data(train, feature_list)\n",
    "        test_input = parse_data(test, feature_list, labels=test.attack.tolist())\n",
    "\n",
    "        cfg = {\n",
    "            \"slide_win\": self.slide_win,\n",
    "            \"slide_stride\": self.slide_stride,\n",
    "        }\n",
    "\n",
    "        train_dataset = TimeDataset(train_input, fc_edge_idx, mode=\"train\", config=cfg)\n",
    "        test_dataset = TimeDataset(test_input, fc_edge_idx, mode=\"test\", config=cfg)\n",
    "\n",
    "        train_subset, validate_subset = self._split_train_validation(train_dataset)\n",
    "\n",
    "        # get data loaders\n",
    "        train_dataloader = DataLoader(train_subset, batch_size=self.batch, shuffle=True)\n",
    "\n",
    "        validate_dataloader = DataLoader(\n",
    "            validate_subset, batch_size=self.batch, shuffle=False\n",
    "        )\n",
    "\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset, batch_size=self.batch, shuffle=False, num_workers=0\n",
    "        )\n",
    "\n",
    "        # save to self\n",
    "        self.fc_edge_idx = fc_edge_idx\n",
    "        self.feature_list = feature_list\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.validate_dataloader = validate_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "\n",
    "    def _load_model(self):\n",
    "        # instantiate model\n",
    "        model = GDN(\n",
    "            self.fc_edge_idx,\n",
    "            n_nodes=len(self.feature_list),\n",
    "            input_dim=self.slide_win,\n",
    "            out_layer_num=self.out_layer_num,\n",
    "            out_layer_inter_dim=self.out_layer_inter_dim,\n",
    "            topk=self.topk,\n",
    "        ).to(self.device)\n",
    "\n",
    "        model._initialise_layers()\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def _get_model_path(self):\n",
    "        # f'./results/{self.data_subdir}/{model_name}.csv'\n",
    "\n",
    "        datestr = datetime.now().strftime(\"%m%d-%H%M%S\")\n",
    "        model_name = datestr if len(self.load_model_name) == 0 else self.load_model_name\n",
    "        model_path = f\"./pretrained/{self.data_subdir}/{model_name}.pt\"\n",
    "        dirname = os.path.dirname(model_path)\n",
    "        Path(dirname).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.model_path = model_path\n",
    "\n",
    "    def _test(self, model, dataloader):\n",
    "\n",
    "        start = datetime.now()\n",
    "\n",
    "        test_loss_list = []\n",
    "        acu_loss = 0\n",
    "\n",
    "        t_test_predicted_list = []\n",
    "        t_test_ground_list = []\n",
    "        t_test_labels_list = []\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for i, (x, y, labels, edge_index) in enumerate(dataloader):\n",
    "            x, y, labels, edge_index = [\n",
    "                item.to(self.device).float() for item in [x, y, labels, edge_index]\n",
    "            ]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                predicted = model(x).float().to(self.device)\n",
    "                loss = loss_func(predicted, y)\n",
    "                labels = labels.unsqueeze(1).repeat(1, predicted.shape[1])\n",
    "\n",
    "                if len(t_test_predicted_list) <= 0:\n",
    "                    t_test_predicted_list = predicted\n",
    "                    t_test_ground_list = y\n",
    "                    t_test_labels_list = labels\n",
    "                else:\n",
    "                    t_test_predicted_list = torch.cat(\n",
    "                        (t_test_predicted_list, predicted), dim=0\n",
    "                    )\n",
    "                    t_test_ground_list = torch.cat((t_test_ground_list, y), dim=0)\n",
    "                    t_test_labels_list = torch.cat((t_test_labels_list, labels), dim=0)\n",
    "\n",
    "            test_loss_list.append(loss.item())\n",
    "            acu_loss += loss.item()\n",
    "\n",
    "            if i % 10000 == 1 and i > 1:\n",
    "                print(str_time_elapsed(start, i, len(dataloader)))\n",
    "\n",
    "        test_predicted_list = t_test_predicted_list.tolist()\n",
    "        test_ground_list = t_test_ground_list.tolist()\n",
    "        test_labels_list = t_test_labels_list.tolist()\n",
    "\n",
    "        avg_loss = sum(test_loss_list) / len(test_loss_list)\n",
    "\n",
    "        return avg_loss, [test_predicted_list, test_ground_list, test_labels_list]\n",
    "\n",
    "    def _train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.lr, weight_decay=self.decay\n",
    "        )\n",
    "\n",
    "        train_log = []\n",
    "        max_loss = 1e8\n",
    "        stop_improve_count = 0\n",
    "\n",
    "        for i_epoch in range(self.epoch):\n",
    "\n",
    "            acu_loss = 0\n",
    "            self.model.train()\n",
    "\n",
    "            for i, (x, labels, _, edge_index) in enumerate(self.train_dataloader):\n",
    "\n",
    "                x, labels, edge_index = [\n",
    "                    item.float().to(self.device) for item in [x, labels, edge_index]\n",
    "                ]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                out = self.model(x).float().to(self.device)\n",
    "\n",
    "                loss = loss_func(out, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_log.append(loss.item())\n",
    "                acu_loss += loss.item()\n",
    "\n",
    "            # each epoch\n",
    "            print(\n",
    "                \"epoch ({} / {}) (Loss:{:.8f}, ACU_loss:{:.8f})\".format(\n",
    "                    i_epoch, self.epoch, acu_loss / (i + 1), acu_loss\n",
    "                ),\n",
    "                flush=True,\n",
    "            )\n",
    "\n",
    "            # use val dataset to judge\n",
    "            if self.validate_dataloader is not None:\n",
    "\n",
    "                val_loss, _ = self._test(self.model, self.validate_dataloader)\n",
    "\n",
    "                if val_loss < max_loss:\n",
    "                    torch.save(self.model.state_dict(), self.model_path)\n",
    "\n",
    "                    max_loss = val_loss\n",
    "                    stop_improve_count = 0\n",
    "                else:\n",
    "                    stop_improve_count += 1\n",
    "\n",
    "                if stop_improve_count >= self.early_stop_win:\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                if acu_loss < max_loss:\n",
    "                    torch.save(self.model.state_dict(), self.model_path)\n",
    "                    max_loss = acu_loss\n",
    "\n",
    "        self.train_log = train_log\n",
    "\n",
    "    def _get_score(self):\n",
    "\n",
    "        # read in best model\n",
    "        self.model.load_state_dict(torch.load(self.model_path))\n",
    "        best_model = self.model.to(self.device)\n",
    "\n",
    "        # store results to self\n",
    "        _, self.test_result = self._test(best_model, self.test_dataloader)\n",
    "        _, self.validate_result = self._test(best_model, self.validate_dataloader)\n",
    "\n",
    "        test_result = np.array(self.test_result)\n",
    "        test_labels = test_result[2, :, 0].tolist()\n",
    "        test_scores = get_full_err_scores(test_result)\n",
    "\n",
    "        info = get_best_performance_data(test_scores, test_labels, topk=1)\n",
    "\n",
    "        print(\"=========================** Result **============================\\n\")\n",
    "        print(f\"F1 score: {info[0]}\")\n",
    "        print(f\"precision: {info[1]}\")\n",
    "        print(f\"recall: {info[2]}\\n\")\n",
    "\n",
    "    def fit(self):\n",
    "        self._load_data()\n",
    "        self._load_model()\n",
    "        self._get_model_path()\n",
    "        self._train()\n",
    "        self._get_score()\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "def loss_func(y_pred, y_true):\n",
    "    return F.mse_loss(y_pred, y_true, reduction=\"mean\")\n",
    "\n",
    "\n",
    "def parse_data(data, feature_list, labels=None):\n",
    "\n",
    "    labels = [0] * data.shape[0] if labels == None else labels\n",
    "    res = data[feature_list].T.values.tolist()\n",
    "    res.append(labels)\n",
    "    return res\n",
    "\n",
    "\n",
    "def str_seconds_to_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def str_time_elapsed(start, i, total):\n",
    "    now = datetime.now()\n",
    "    elapsed = (now - start).seconds\n",
    "    frac_complete = (i + 1) / total\n",
    "    remaining = elapsed / frac_complete - elapsed\n",
    "    return \"%s (- %s)\" % (\n",
    "        str_seconds_to_minutes(elapsed),\n",
    "        str_seconds_to_minutes(remaining),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_full_err_scores(test_result):\n",
    "    test_result = np.array(test_result)\n",
    "\n",
    "    all_scores = None\n",
    "    feature_num = test_result.shape[-1]\n",
    "\n",
    "    for i in range(feature_num):\n",
    "        test_result_list = test_result[:2, :, i]\n",
    "        scores = get_err_scores(test_result_list)\n",
    "\n",
    "        if all_scores is None:\n",
    "            all_scores = scores\n",
    "        else:\n",
    "            all_scores = np.vstack((all_scores, scores))\n",
    "\n",
    "    return all_scores\n",
    "\n",
    "\n",
    "def get_err_scores(test_result_list):\n",
    "    test_predict, test_ground = test_result_list\n",
    "\n",
    "    n_err_mid, n_err_iqr = get_err_median_and_iqr(test_predict, test_ground)\n",
    "\n",
    "    test_delta = np.abs(\n",
    "        np.subtract(\n",
    "            np.array(test_predict).astype(np.float64),\n",
    "            np.array(test_ground).astype(np.float64),\n",
    "        )\n",
    "    )\n",
    "    epsilon = 1e-2\n",
    "\n",
    "    err_scores = (test_delta - n_err_mid) / (np.abs(n_err_iqr) + epsilon)\n",
    "\n",
    "    smoothed_err_scores = np.zeros(err_scores.shape)\n",
    "    before_num = 3\n",
    "    for i in range(before_num, len(err_scores)):\n",
    "        smoothed_err_scores[i] = np.mean(err_scores[i - before_num : i + 1])\n",
    "\n",
    "    return smoothed_err_scores\n",
    "\n",
    "\n",
    "def get_err_median_and_iqr(predicted, groundtruth):\n",
    "\n",
    "    np_arr = np.abs(np.subtract(np.array(predicted), np.array(groundtruth)))\n",
    "\n",
    "    err_median = np.median(np_arr)\n",
    "    err_iqr = iqr(np_arr)\n",
    "\n",
    "    return err_median, err_iqr\n",
    "\n",
    "\n",
    "def get_best_performance_data(total_err_scores, gt_labels, topk=1):\n",
    "\n",
    "    total_features = total_err_scores.shape[0]\n",
    "\n",
    "    topk_indices = np.argpartition(\n",
    "        total_err_scores, range(total_features - topk - 1, total_features), axis=0\n",
    "    )[-topk:]\n",
    "\n",
    "    total_topk_err_scores = []\n",
    "\n",
    "    total_topk_err_scores = np.sum(\n",
    "        np.take_along_axis(total_err_scores, topk_indices, axis=0), axis=0\n",
    "    )\n",
    "\n",
    "    final_topk_fmeas, thresolds = eval_scores(\n",
    "        total_topk_err_scores, gt_labels, 400, return_thresold=True\n",
    "    )\n",
    "\n",
    "    th_i = final_topk_fmeas.index(max(final_topk_fmeas))\n",
    "    thresold = thresolds[th_i]\n",
    "\n",
    "    pred_labels = np.zeros(len(total_topk_err_scores))\n",
    "    pred_labels[total_topk_err_scores > thresold] = 1\n",
    "\n",
    "    for i in range(len(pred_labels)):\n",
    "        pred_labels[i] = int(pred_labels[i])\n",
    "        gt_labels[i] = int(gt_labels[i])\n",
    "\n",
    "    pre = precision_score(gt_labels, pred_labels)\n",
    "    rec = recall_score(gt_labels, pred_labels)\n",
    "\n",
    "    auc_score = roc_auc_score(gt_labels, total_topk_err_scores)\n",
    "\n",
    "    return max(final_topk_fmeas), pre, rec, auc_score, thresold\n",
    "\n",
    "\n",
    "# calculate F1 scores\n",
    "def eval_scores(scores, true_scores, th_steps, return_thresold=False):\n",
    "    padding_list = [0] * (len(true_scores) - len(scores))\n",
    "\n",
    "    if len(padding_list) > 0:\n",
    "        scores = padding_list + scores\n",
    "\n",
    "    scores_sorted = rankdata(scores, method=\"ordinal\")\n",
    "    th_vals = np.array(range(th_steps)) * 1.0 / th_steps\n",
    "    fmeas = [None] * th_steps\n",
    "    thresholds = [None] * th_steps\n",
    "    \n",
    "    for i in range(th_steps):\n",
    "        cur_pred = scores_sorted > th_vals[i] * len(scores)\n",
    "\n",
    "        fmeas[i] = f1_score(true_scores, cur_pred)\n",
    "\n",
    "        score_index = scores_sorted.tolist().index(int(th_vals[i] * len(scores) + 1))\n",
    "        thresholds[i] = scores[score_index]\n",
    "\n",
    "    if return_thresold:\n",
    "        return fmeas, thresholds\n",
    "    return fmeas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch (0 / 100) (Loss:0.86045003, ACU_loss:2.58135009)\n",
      "epoch (1 / 100) (Loss:0.75903294, ACU_loss:2.27709883)\n",
      "epoch (2 / 100) (Loss:0.67835351, ACU_loss:2.03506052)\n",
      "epoch (3 / 100) (Loss:0.63620541, ACU_loss:1.90861624)\n",
      "epoch (4 / 100) (Loss:0.59909836, ACU_loss:1.79729509)\n",
      "epoch (5 / 100) (Loss:0.57063081, ACU_loss:1.71189243)\n",
      "epoch (6 / 100) (Loss:0.53735771, ACU_loss:1.61207312)\n",
      "epoch (7 / 100) (Loss:0.50432665, ACU_loss:1.51297995)\n",
      "epoch (8 / 100) (Loss:0.46246579, ACU_loss:1.38739738)\n",
      "epoch (9 / 100) (Loss:0.43439284, ACU_loss:1.30317852)\n",
      "epoch (10 / 100) (Loss:0.39554287, ACU_loss:1.18662861)\n",
      "epoch (11 / 100) (Loss:0.38591270, ACU_loss:1.15773809)\n",
      "epoch (12 / 100) (Loss:0.35697809, ACU_loss:1.07093427)\n",
      "epoch (13 / 100) (Loss:0.34683675, ACU_loss:1.04051024)\n",
      "epoch (14 / 100) (Loss:0.32587393, ACU_loss:0.97762179)\n",
      "epoch (15 / 100) (Loss:0.36665658, ACU_loss:1.09996974)\n",
      "epoch (16 / 100) (Loss:0.33465432, ACU_loss:1.00396296)\n",
      "epoch (17 / 100) (Loss:0.30046485, ACU_loss:0.90139455)\n",
      "epoch (18 / 100) (Loss:0.29832293, ACU_loss:0.89496878)\n",
      "epoch (19 / 100) (Loss:0.28647705, ACU_loss:0.85943115)\n",
      "epoch (20 / 100) (Loss:0.27932840, ACU_loss:0.83798519)\n",
      "epoch (21 / 100) (Loss:0.27304131, ACU_loss:0.81912394)\n",
      "epoch (22 / 100) (Loss:0.26210108, ACU_loss:0.78630325)\n",
      "epoch (23 / 100) (Loss:0.27451929, ACU_loss:0.82355788)\n",
      "epoch (24 / 100) (Loss:0.26704021, ACU_loss:0.80112064)\n",
      "epoch (25 / 100) (Loss:0.26096658, ACU_loss:0.78289974)\n",
      "epoch (26 / 100) (Loss:0.24903539, ACU_loss:0.74710616)\n",
      "epoch (27 / 100) (Loss:0.25757598, ACU_loss:0.77272795)\n",
      "epoch (28 / 100) (Loss:0.24082225, ACU_loss:0.72246674)\n",
      "epoch (29 / 100) (Loss:0.24541938, ACU_loss:0.73625815)\n",
      "epoch (30 / 100) (Loss:0.25697544, ACU_loss:0.77092633)\n",
      "epoch (31 / 100) (Loss:0.24626020, ACU_loss:0.73878059)\n",
      "epoch (32 / 100) (Loss:0.23730987, ACU_loss:0.71192960)\n",
      "epoch (33 / 100) (Loss:0.22154035, ACU_loss:0.66462106)\n",
      "epoch (34 / 100) (Loss:0.26008627, ACU_loss:0.78025882)\n",
      "epoch (35 / 100) (Loss:0.22157936, ACU_loss:0.66473809)\n",
      "epoch (36 / 100) (Loss:0.22473807, ACU_loss:0.67421420)\n",
      "epoch (37 / 100) (Loss:0.22366315, ACU_loss:0.67098945)\n",
      "epoch (38 / 100) (Loss:0.21336359, ACU_loss:0.64009076)\n",
      "epoch (39 / 100) (Loss:0.21273187, ACU_loss:0.63819560)\n",
      "epoch (40 / 100) (Loss:0.24776796, ACU_loss:0.74330389)\n",
      "epoch (41 / 100) (Loss:0.20701071, ACU_loss:0.62103213)\n",
      "epoch (42 / 100) (Loss:0.21178606, ACU_loss:0.63535817)\n",
      "epoch (43 / 100) (Loss:0.23811332, ACU_loss:0.71433996)\n",
      "epoch (44 / 100) (Loss:0.24459569, ACU_loss:0.73378707)\n",
      "epoch (45 / 100) (Loss:0.20904312, ACU_loss:0.62712936)\n",
      "epoch (46 / 100) (Loss:0.19587907, ACU_loss:0.58763722)\n",
      "epoch (47 / 100) (Loss:0.20365826, ACU_loss:0.61097479)\n",
      "epoch (48 / 100) (Loss:0.20889039, ACU_loss:0.62667117)\n",
      "epoch (49 / 100) (Loss:0.20888474, ACU_loss:0.62665422)\n",
      "epoch (50 / 100) (Loss:0.20021584, ACU_loss:0.60064752)\n",
      "epoch (51 / 100) (Loss:0.20316934, ACU_loss:0.60950801)\n",
      "epoch (52 / 100) (Loss:0.18784900, ACU_loss:0.56354700)\n",
      "epoch (53 / 100) (Loss:0.23646723, ACU_loss:0.70940170)\n",
      "epoch (54 / 100) (Loss:0.19146014, ACU_loss:0.57438041)\n",
      "epoch (55 / 100) (Loss:0.17954500, ACU_loss:0.53863500)\n",
      "epoch (56 / 100) (Loss:0.19665918, ACU_loss:0.58997755)\n",
      "epoch (57 / 100) (Loss:0.20575253, ACU_loss:0.61725760)\n",
      "epoch (58 / 100) (Loss:0.18033477, ACU_loss:0.54100430)\n",
      "epoch (59 / 100) (Loss:0.19579137, ACU_loss:0.58737412)\n",
      "epoch (60 / 100) (Loss:0.18206991, ACU_loss:0.54620974)\n",
      "epoch (61 / 100) (Loss:0.19879714, ACU_loss:0.59639141)\n",
      "epoch (62 / 100) (Loss:0.18567266, ACU_loss:0.55701798)\n",
      "epoch (63 / 100) (Loss:0.18654127, ACU_loss:0.55962382)\n",
      "epoch (64 / 100) (Loss:0.19008536, ACU_loss:0.57025608)\n",
      "epoch (65 / 100) (Loss:0.24341322, ACU_loss:0.73023966)\n",
      "epoch (66 / 100) (Loss:0.18028193, ACU_loss:0.54084580)\n",
      "epoch (67 / 100) (Loss:0.18271505, ACU_loss:0.54814516)\n",
      "epoch (68 / 100) (Loss:0.18335049, ACU_loss:0.55005148)\n",
      "epoch (69 / 100) (Loss:0.21396834, ACU_loss:0.64190502)\n",
      "epoch (70 / 100) (Loss:0.17210165, ACU_loss:0.51630495)\n",
      "epoch (71 / 100) (Loss:0.17167323, ACU_loss:0.51501970)\n",
      "epoch (72 / 100) (Loss:0.17220762, ACU_loss:0.51662287)\n",
      "epoch (73 / 100) (Loss:0.19691987, ACU_loss:0.59075962)\n",
      "epoch (74 / 100) (Loss:0.18846057, ACU_loss:0.56538172)\n",
      "epoch (75 / 100) (Loss:0.16184726, ACU_loss:0.48554179)\n",
      "epoch (76 / 100) (Loss:0.23236470, ACU_loss:0.69709410)\n",
      "epoch (77 / 100) (Loss:0.15978972, ACU_loss:0.47936915)\n",
      "epoch (78 / 100) (Loss:0.17532662, ACU_loss:0.52597985)\n",
      "epoch (79 / 100) (Loss:0.25714752, ACU_loss:0.77144256)\n",
      "epoch (80 / 100) (Loss:0.16379966, ACU_loss:0.49139899)\n",
      "epoch (81 / 100) (Loss:0.17617864, ACU_loss:0.52853592)\n",
      "epoch (82 / 100) (Loss:0.17305530, ACU_loss:0.51916590)\n",
      "epoch (83 / 100) (Loss:0.16865833, ACU_loss:0.50597499)\n",
      "epoch (84 / 100) (Loss:0.19011297, ACU_loss:0.57033892)\n",
      "epoch (85 / 100) (Loss:0.16854658, ACU_loss:0.50563973)\n",
      "epoch (86 / 100) (Loss:0.16150446, ACU_loss:0.48451339)\n",
      "epoch (87 / 100) (Loss:0.17696858, ACU_loss:0.53090575)\n",
      "epoch (88 / 100) (Loss:0.16535831, ACU_loss:0.49607493)\n",
      "epoch (89 / 100) (Loss:0.17367727, ACU_loss:0.52103181)\n",
      "epoch (90 / 100) (Loss:0.16929426, ACU_loss:0.50788277)\n",
      "epoch (91 / 100) (Loss:0.15708323, ACU_loss:0.47124970)\n",
      "=========================** Result **============================\n",
      "\n",
      "F1 score: 0.8812344998622209\n",
      "precision: 0.7884615384615384\n",
      "recall: 0.999375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GNNAD()\n",
    "fitted_model = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python main.py -dataset msl -device cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('gdn_old')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99f10b4e66e58daffdd3587f8013f35f7438a354bd317537c02f690a0bc2561c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
