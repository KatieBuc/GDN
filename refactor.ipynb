{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops, softmax\n",
    "\n",
    "from datasets.TimeDataset import TimeDataset\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLayer(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        heads=1,\n",
    "        concat=True,\n",
    "        negative_slope=0.2,\n",
    "        dropout=0,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super(GraphLayer, self).__init__(aggr=\"add\")\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.__alpha__ = None\n",
    "\n",
    "        self.lin = Linear(in_channels, heads * out_channels, bias=False)\n",
    "\n",
    "        self.att_i = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_j = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_em_i = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_em_j = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.lin.weight)\n",
    "        glorot(self.att_i)\n",
    "        glorot(self.att_j)\n",
    "        zeros(self.att_em_i)\n",
    "        zeros(self.att_em_j)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, embedding, return_attention_weights=False):\n",
    "\n",
    "        if torch.is_tensor(x):\n",
    "            x = self.lin(x)\n",
    "            x = (x, x)\n",
    "        else:\n",
    "            x = (self.lin(x[0]), self.lin(x[1]))\n",
    "\n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x[1].size(self.node_dim))\n",
    "\n",
    "        out = self.propagate(\n",
    "            edge_index,\n",
    "            x=x,\n",
    "            embedding=embedding,\n",
    "            edges=edge_index,\n",
    "            return_attention_weights=return_attention_weights,\n",
    "        )\n",
    "\n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        if return_attention_weights:\n",
    "            alpha, self.__alpha__ = self.__alpha__, None\n",
    "            return out, (edge_index, alpha)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def message(\n",
    "        self, x_i, x_j, edge_index_i, size_i, embedding, edges, return_attention_weights\n",
    "    ):\n",
    "\n",
    "        x_i = x_i.view(-1, self.heads, self.out_channels)\n",
    "        x_j = x_j.view(-1, self.heads, self.out_channels)\n",
    "\n",
    "        if embedding is not None:\n",
    "            embedding_i, embedding_j = embedding[edge_index_i], embedding[edges[0]]\n",
    "            embedding_i = embedding_i.unsqueeze(1).repeat(1, self.heads, 1)\n",
    "            embedding_j = embedding_j.unsqueeze(1).repeat(1, self.heads, 1)\n",
    "\n",
    "            key_i = torch.cat(\n",
    "                (x_i, embedding_i), dim=-1\n",
    "            )  # key_i's are the g_i's, does x_i already have W?\n",
    "            key_j = torch.cat(\n",
    "                (x_j, embedding_j), dim=-1\n",
    "            )  # concatenates along the last dim, i.e. columns in this case\n",
    "\n",
    "        cat_att_i = torch.cat((self.att_i, self.att_em_i), dim=-1)\n",
    "        cat_att_j = torch.cat((self.att_j, self.att_em_j), dim=-1)\n",
    "\n",
    "        alpha = (key_i * cat_att_i).sum(-1) + (key_j * cat_att_j).sum(\n",
    "            -1\n",
    "        )  # eqn (6) but...\n",
    "        alpha = alpha.view(-1, self.heads, 1)\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)  # eqn (7)\n",
    "        alpha = softmax(alpha, edge_index_i, size_i)  # eqn (8)\n",
    "\n",
    "        if return_attention_weights:\n",
    "            self.__alpha__ = alpha\n",
    "\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "\n",
    "        return x_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}({}, {}, heads={})\".format(\n",
    "            self.__class__.__name__, self.in_channels, self.out_channels, self.heads\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutLayer(nn.Module):\n",
    "    def __init__(self, in_num, layer_num, inter_num=512):\n",
    "\n",
    "        super(OutLayer, self).__init__()\n",
    "        modules = []\n",
    "        for i in range(layer_num):\n",
    "            if i == layer_num - 1:\n",
    "                modules.append(nn.Linear(in_num if layer_num == 1 else inter_num, 1))\n",
    "            else:\n",
    "                layer_in_num = in_num if i == 0 else inter_num\n",
    "                modules.append(nn.Linear(layer_in_num, inter_num))\n",
    "                modules.append(nn.BatchNorm1d(inter_num))\n",
    "                modules.append(nn.ReLU())\n",
    "\n",
    "        self.mlp = nn.ModuleList(modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        for mod in self.mlp:\n",
    "            if isinstance(mod, nn.BatchNorm1d):\n",
    "                out = out.permute(0, 2, 1)\n",
    "                out = mod(out)\n",
    "                out = out.permute(0, 2, 1)\n",
    "            else:\n",
    "                out = mod(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, heads=1):\n",
    "        super(GNNLayer, self).__init__()\n",
    "\n",
    "        self.gnn = GraphLayer(in_channel, out_channel, heads=heads, concat=False)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, embedding=None):\n",
    "\n",
    "        out, (new_edge_index, att_weight) = self.gnn(\n",
    "            x, edge_index, embedding, return_attention_weights=True\n",
    "        )\n",
    "\n",
    "        self.att_weight_1 = att_weight\n",
    "        self.edge_index_1 = new_edge_index\n",
    "\n",
    "        out = self.bn(out)\n",
    "\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class GDN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fc_edge_idx,\n",
    "        n_nodes,\n",
    "        embed_dim=64,\n",
    "        out_layer_inter_dim=256,\n",
    "        input_dim=10,\n",
    "        out_layer_num=1,\n",
    "        topk=20,\n",
    "    ):\n",
    "        super(GDN, self).__init__()\n",
    "\n",
    "        self.fc_edge_idx = fc_edge_idx\n",
    "        self.n_nodes = n_nodes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.out_layer_inter_dim = out_layer_inter_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.out_layer_num = out_layer_num\n",
    "        self.topk = topk\n",
    "\n",
    "    def _initialise_layers(self):\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_nodes, self.embed_dim)\n",
    "        nn.init.kaiming_uniform_(self.embedding.weight, a=math.sqrt(5))\n",
    "\n",
    "        self.bn_outlayer_in = nn.BatchNorm1d(self.embed_dim)\n",
    "\n",
    "        self.gnn_layers = nn.ModuleList(\n",
    "            [\n",
    "                GNNLayer(\n",
    "                    self.input_dim,\n",
    "                    self.embed_dim,\n",
    "                    heads=1,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.node_embedding = None\n",
    "        self.learned_graph = None\n",
    "\n",
    "        self.out_layer = OutLayer(\n",
    "            self.embed_dim, self.out_layer_num, inter_num=self.out_layer_inter_dim\n",
    "        )\n",
    "\n",
    "        self.cache_fc_edge_idx = None\n",
    "        self.cache_embed_index = None\n",
    "\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x = data.clone().detach()\n",
    "        device = data.device\n",
    "        batch_num, n_nodes, all_feature = x.shape\n",
    "        x = x.view(-1, all_feature).contiguous()\n",
    "\n",
    "        if self.cache_fc_edge_idx is None:\n",
    "            self.cache_fc_edge_idx = get_batch_edge_index(\n",
    "                self.fc_edge_idx, batch_num, n_nodes\n",
    "            ).to(device)\n",
    "\n",
    "        all_embeddings = self.embedding(torch.arange(n_nodes).to(device))  # v_i's\n",
    "\n",
    "        weights_arr = all_embeddings.detach().clone()\n",
    "        all_embeddings = all_embeddings.repeat(batch_num, 1)\n",
    "\n",
    "        weights = weights_arr.view(n_nodes, -1)\n",
    "\n",
    "        cos_ji_mat = torch.matmul(weights, weights.T)  # e_{ji} in eqn (2)\n",
    "        normed_mat = torch.matmul(\n",
    "            weights.norm(dim=-1).view(-1, 1), weights.norm(dim=-1).view(1, -1)\n",
    "        )\n",
    "        cos_ji_mat = cos_ji_mat / normed_mat\n",
    "\n",
    "        topk_indices_ji = torch.topk(cos_ji_mat, self.topk, dim=-1)[\n",
    "            1\n",
    "        ]  # A_{ji} in eqn (3)\n",
    "\n",
    "        self.learned_graph = topk_indices_ji\n",
    "\n",
    "        gated_i_ = torch.arange(0, n_nodes)\n",
    "        gated_i = (\n",
    "            gated_i_.permute(*torch.arange(gated_i_.ndim - 1, -1, -1))\n",
    "            .unsqueeze(1)\n",
    "            .repeat(1, self.topk)\n",
    "            .flatten()\n",
    "            .to(device)\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        gated_j = topk_indices_ji.flatten().unsqueeze(0)\n",
    "        gated_edge_index = torch.cat((gated_j, gated_i), dim=0)\n",
    "\n",
    "        batch_gated_edge_index = get_batch_edge_index(\n",
    "            gated_edge_index, batch_num, n_nodes\n",
    "        ).to(device)\n",
    "\n",
    "        gcn_out = self.gnn_layers[0](\n",
    "            x,\n",
    "            batch_gated_edge_index,\n",
    "            embedding=all_embeddings,\n",
    "        )\n",
    "        gcn_out = gcn_out.view(batch_num, n_nodes, -1)\n",
    "\n",
    "        idxs = torch.arange(0, n_nodes).to(device)\n",
    "        out = torch.mul(gcn_out, self.embedding(idxs))\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = F.relu(self.bn_outlayer_in(out))  # eqn (5)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.dp(out)\n",
    "        out = self.out_layer(out)\n",
    "        out = out.view(-1, n_nodes)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_batch_edge_index(org_edge_index, batch_num, n_nodes):\n",
    "\n",
    "    edge_index = org_edge_index.clone().detach()\n",
    "    edge_num = org_edge_index.shape[1]\n",
    "    batch_edge_index = edge_index.repeat(1, batch_num).contiguous()\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        batch_edge_index[:, i * edge_num : (i + 1) * edge_num] += i * n_nodes\n",
    "\n",
    "    return batch_edge_index.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor to module\n",
    "#    - run: test, get score (GNNAD) [today]\n",
    "#    - check output matches, write unit tests [tonight]\n",
    "\n",
    "# --------------------\n",
    "# CELEBRATE!\n",
    "# --------------------\n",
    "\n",
    "# plots like paper\n",
    "# error handling for real data!\n",
    "# get interactive validation screen (notebook)\n",
    "# make this work for new python/torch verison\n",
    "# ...\n",
    "# ideas for research (graph metrics, input node-related anomaly) [meet with Rob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNAD:\n",
    "    \"\"\"\n",
    "    Graph Neural Network-based Anomaly Detection in Multivariate Timeseries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch: int = 128,\n",
    "        epoch: int = 100,\n",
    "        slide_win: int = 15,\n",
    "        dim: int = 64,\n",
    "        slide_stride: int = 5,\n",
    "        comment: str = \"\",\n",
    "        random_seed: int = 0,\n",
    "        out_layer_num: int = 1,\n",
    "        out_layer_inter_dim: int = 256,\n",
    "        decay: float = 0,\n",
    "        validate_ratio: float = 0.1,\n",
    "        topk: int = 20,\n",
    "        data_subdir: str = \"msl\",\n",
    "        device: str = \"cpu\",\n",
    "        report: str = \"best\",\n",
    "        load_model_name: str = \"\",\n",
    "        early_stop_win: int = 15,\n",
    "        lr: float = 0.001,\n",
    "    ):\n",
    "\n",
    "        self.batch = batch\n",
    "        self.epoch = epoch\n",
    "        self.slide_win = slide_win\n",
    "        self.dim = dim\n",
    "        self.slide_stride = slide_stride\n",
    "        self.comment = comment\n",
    "        self.random_seed = random_seed\n",
    "        self.out_layer_num = out_layer_num\n",
    "        self.out_layer_inter_dim = out_layer_inter_dim\n",
    "        self.decay = decay\n",
    "        self.validate_ratio = validate_ratio\n",
    "        self.topk = topk\n",
    "        self.data_subdir = data_subdir\n",
    "        self.device = device\n",
    "        self.report = report\n",
    "        self.load_model_name = load_model_name\n",
    "        self.early_stop_win = early_stop_win\n",
    "        self.lr = lr\n",
    "\n",
    "    def _split_train_validation(self, data):\n",
    "\n",
    "        dataset_len = len(data)\n",
    "        validate_use_len = int(dataset_len * self.validate_ratio)\n",
    "        validate_start_idx = random.randrange(dataset_len - validate_use_len)\n",
    "        idx = torch.arange(dataset_len)\n",
    "\n",
    "        train_sub_idx = torch.cat(\n",
    "            [idx[:validate_start_idx], idx[validate_start_idx + validate_use_len :]]\n",
    "        )\n",
    "        train_subset = Subset(data, train_sub_idx)\n",
    "\n",
    "        validate_sub_idx = idx[\n",
    "            validate_start_idx : validate_start_idx + validate_use_len\n",
    "        ]\n",
    "        validate_subset = Subset(data, validate_sub_idx)\n",
    "\n",
    "        return train_subset, validate_subset\n",
    "\n",
    "    def _load_data(self):\n",
    "\n",
    "        train = pd.read_csv(\n",
    "            f\"./data/{self.data_subdir}/train.csv\", sep=\",\", index_col=0\n",
    "        )\n",
    "        test = pd.read_csv(f\"./data/{self.data_subdir}/test.csv\", sep=\",\", index_col=0)\n",
    "\n",
    "        train = train.drop(columns=[\"attack\"]) if \"attack\" in train.columns else train\n",
    "\n",
    "        feature_list = train.columns[\n",
    "            train.columns.str[0] != \"_\"\n",
    "        ].to_list()  # convention is to pass non-features as '_'\n",
    "        assert len(feature_list) == len(set(feature_list))\n",
    "\n",
    "        fc_struc = {\n",
    "            ft: [x for x in feature_list if x != ft] for ft in feature_list\n",
    "        }  # fully connected structure\n",
    "\n",
    "        edge__idx_tuples = [\n",
    "            (feature_list.index(child), feature_list.index(node_name))\n",
    "            for node_name, node_list in fc_struc.items()\n",
    "            for child in node_list\n",
    "        ]\n",
    "\n",
    "        fc_edge_idx = [\n",
    "            [x[0] for x in edge__idx_tuples],\n",
    "            [x[1] for x in edge__idx_tuples],\n",
    "        ]\n",
    "        fc_edge_idx = torch.tensor(fc_edge_idx, dtype=torch.long)\n",
    "\n",
    "        train_input = parse_data(train, feature_list)\n",
    "        test_input = parse_data(test, feature_list, labels=test.attack.tolist())\n",
    "\n",
    "        cfg = {\n",
    "            \"slide_win\": self.slide_win,\n",
    "            \"slide_stride\": self.slide_stride,\n",
    "        }\n",
    "\n",
    "        train_dataset = TimeDataset(train_input, fc_edge_idx, mode=\"train\", config=cfg)\n",
    "        test_dataset = TimeDataset(test_input, fc_edge_idx, mode=\"test\", config=cfg)\n",
    "\n",
    "        train_subset, validate_subset = self._split_train_validation(train_dataset)\n",
    "\n",
    "        # get data loaders\n",
    "        train_dataloader = DataLoader(train_subset, batch_size=self.batch, shuffle=True)\n",
    "\n",
    "        validate_dataloader = DataLoader(\n",
    "            validate_subset, batch_size=self.batch, shuffle=False\n",
    "        )\n",
    "\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset, batch_size=self.batch, shuffle=False, num_workers=0\n",
    "        )\n",
    "\n",
    "        # save to self\n",
    "        self.fc_edge_idx = fc_edge_idx\n",
    "        self.feature_list = feature_list\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.validate_dataloader = validate_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "\n",
    "    def _load_model(self):\n",
    "        # instantiate model\n",
    "        model = GDN(\n",
    "            self.fc_edge_idx,\n",
    "            n_nodes=len(self.feature_list),\n",
    "            input_dim=self.slide_win,\n",
    "            out_layer_num=self.out_layer_num,\n",
    "            out_layer_inter_dim=self.out_layer_inter_dim,\n",
    "            topk=self.topk,\n",
    "        ).to(self.device)\n",
    "\n",
    "        model._initialise_layers()\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def _get_model_path(self):\n",
    "        # f'./results/{self.data_subdir}/{model_name}.csv'\n",
    "\n",
    "        datestr = datetime.now().strftime(\"%m%d-%H%M%S\")\n",
    "        model_name = datestr if len(self.load_model_name) == 0 else self.load_model_name\n",
    "        model_path = f\"./pretrained/{self.data_subdir}/{model_name}.pt\"\n",
    "        dirname = os.path.dirname(model_path)\n",
    "        Path(dirname).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.model_path = model_path\n",
    "\n",
    "    def _test(self, dataloader):\n",
    "\n",
    "        start = datetime.now()\n",
    "\n",
    "        test_loss_list = []\n",
    "        acu_loss = 0\n",
    "\n",
    "        t_test_predicted_list = []\n",
    "        t_test_ground_list = []\n",
    "        t_test_labels_list = []\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        for i, (x, y, labels, edge_index) in enumerate(dataloader):\n",
    "            x, y, labels, edge_index = [\n",
    "                item.to(self.device).float() for item in [x, y, labels, edge_index]\n",
    "            ]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                predicted = self.model(x).float().to(self.device)\n",
    "                loss = loss_func(predicted, y)\n",
    "                labels = labels.unsqueeze(1).repeat(1, predicted.shape[1])\n",
    "\n",
    "                if len(t_test_predicted_list) <= 0:\n",
    "                    t_test_predicted_list = predicted\n",
    "                    t_test_ground_list = y\n",
    "                    t_test_labels_list = labels\n",
    "                else:\n",
    "                    t_test_predicted_list = torch.cat(\n",
    "                        (t_test_predicted_list, predicted), dim=0\n",
    "                    )\n",
    "                    t_test_ground_list = torch.cat((t_test_ground_list, y), dim=0)\n",
    "                    t_test_labels_list = torch.cat((t_test_labels_list, labels), dim=0)\n",
    "\n",
    "            test_loss_list.append(loss.item())\n",
    "            acu_loss += loss.item()\n",
    "\n",
    "            if i % 10000 == 1 and i > 1:\n",
    "                print(str_time_elapsed(start, i, len(dataloader)))\n",
    "\n",
    "        test_predicted_list = t_test_predicted_list.tolist()\n",
    "        test_ground_list = t_test_ground_list.tolist()\n",
    "        test_labels_list = t_test_labels_list.tolist()\n",
    "\n",
    "        avg_loss = sum(test_loss_list) / len(test_loss_list)\n",
    "\n",
    "        return avg_loss, [test_predicted_list, test_ground_list, test_labels_list]\n",
    "\n",
    "    def _train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.lr, weight_decay=self.decay\n",
    "        )\n",
    "\n",
    "        train_log = []\n",
    "        max_loss = 1e8\n",
    "        stop_improve_count = 0\n",
    "\n",
    "        for i_epoch in range(self.epoch):\n",
    "\n",
    "            acu_loss = 0\n",
    "            self.model.train()\n",
    "\n",
    "            for i, (x, labels, _, edge_index) in enumerate(self.train_dataloader):\n",
    "\n",
    "                x, labels, edge_index = [\n",
    "                    item.float().to(self.device) for item in [x, labels, edge_index]\n",
    "                ]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                out = self.model(x).float().to(self.device)\n",
    "\n",
    "                loss = loss_func(out, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_log.append(loss.item())\n",
    "                acu_loss += loss.item()\n",
    "\n",
    "            # each epoch\n",
    "            print(\n",
    "                \"epoch ({} / {}) (Loss:{:.8f}, ACU_loss:{:.8f})\".format(\n",
    "                    i_epoch, self.epoch, acu_loss / (i + 1), acu_loss\n",
    "                ),\n",
    "                flush=True,\n",
    "            )\n",
    "\n",
    "            # use val dataset to judge\n",
    "            if self.validate_dataloader is not None:\n",
    "\n",
    "                val_loss, _ = self._test(self.validate_dataloader)\n",
    "\n",
    "                if val_loss < max_loss:\n",
    "                    torch.save(self.model.state_dict(), self.model_path)\n",
    "\n",
    "                    max_loss = val_loss\n",
    "                    stop_improve_count = 0\n",
    "                else:\n",
    "                    stop_improve_count += 1\n",
    "\n",
    "                if stop_improve_count >= self.early_stop_win:\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                if acu_loss < max_loss:\n",
    "                    torch.save(self.model.state_dict(), self.model_path)\n",
    "                    max_loss = acu_loss\n",
    "\n",
    "        self.train_log = train_log\n",
    "\n",
    "    def fit(self):\n",
    "        self._load_data()\n",
    "        self._load_model()\n",
    "        self._get_model_path()\n",
    "        self._train()\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "def loss_func(y_pred, y_true):\n",
    "    return F.mse_loss(y_pred, y_true, reduction=\"mean\")\n",
    "\n",
    "\n",
    "def parse_data(data, feature_list, labels=None):\n",
    "\n",
    "    labels = [0] * data.shape[0] if labels == None else labels\n",
    "    res = data[feature_list].T.values.tolist()\n",
    "    res.append(labels)\n",
    "    return res\n",
    "\n",
    "\n",
    "def str_seconds_to_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def str_time_elapsed(start, i, total):\n",
    "    now = datetime.now()\n",
    "    elapsed = (now - start).seconds\n",
    "    frac_complete = (i + 1) / total\n",
    "    remaining = elapsed / frac_complete - elapsed\n",
    "    return \"%s (- %s)\" % (\n",
    "        str_seconds_to_minutes(elapsed),\n",
    "        str_seconds_to_minutes(remaining),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNNAD()\n",
    "fitted_model = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python main.py -dataset msl -device cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('gdn_old')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99f10b4e66e58daffdd3587f8013f35f7438a354bd317537c02f690a0bc2561c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
